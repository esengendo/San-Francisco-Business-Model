# San Francisco Business Success Prediction Platform

## üéØ **Executive Summary**

An end-to-end **deep learning solution** that predicts **5-year business survival probability** for San Francisco enterprises using **multi-source data integration** and **PyTorch neural networks**. This production-ready system processes **250K+ business records** from **13 distinct API sources** to deliver actionable insights for entrepreneurial decision-making.

## üöÄ **Live Demo**
```bash
docker run -p 8501:8501 esengendo730/sf-business-model:latest
```
**Access**: http://localhost:8501

---

## üè¢ **Business Problem & Impact**

**Problem**: San Francisco entrepreneurs lack data-driven insights to assess business viability across different locations, timeframes, and economic conditions.

**Solution**: A comprehensive ML platform that analyzes **118 engineered features** across spatial, temporal, and economic dimensions to predict business success probability after 5+ years of operation.

**Business Value**:
- **Risk Assessment**: Quantify location-specific business risks
- **Strategic Planning**: Optimize timing and location decisions
- **Resource Allocation**: Focus investments on high-probability ventures
- **Market Intelligence**: Understand economic trend impacts on business survival

---

## üî¨ **Technical Architecture**

### **Deep Learning Model**
- **Framework**: PyTorch Lightning with custom neural architecture
- **Input Features**: 118 engineered features (5 spatial, 1 temporal, 112 business/economic)
- **Architecture**: Multi-branch neural network with specialized feature processing
- **Target**: Binary classification (5-year business survival)
- **Performance**: Optimized for real-world deployment with robust error handling

### **Data Engineering Pipeline**
**13 API Data Sources** integrated through automated ETL:

1. **San Francisco Open Data** (`sf_business_data_04.py`) - Core business registrations
2. **FRED Economic API** (`fred_economic_data_05.py`) - GDP, unemployment, housing indices
3. **Census Demographics** (`census_demographic_data_06.py`) - Population and economic indicators
4. **SF Planning Department** (`sf_planning_data_07.py`) - Zoning and development data
5. **SF Crime Data** (`sf_crime_data_08.py`) - Public safety indicators
6. **SF 311 Services** (`sf311_data_09.py`) - City service requests and quality metrics
7. **OpenStreetMap** (`osm_business_data_10.py`) - Geospatial business density
8. **GDELT News** (`gdelt_news_data_11.py`) - Economic sentiment analysis
9. **RSS News Feeds** (`sf_news_rss_data_12.py`) - Local business news sentiment
10. **Wayback Machine** (`wayback_historical_data_13.py`) - Historical business presence
11. **Land Use Integration** (`land_use_integration_merge_18.py`) - Zoning optimization
12. **Permits Integration** (`permits_integration_merge_19.py`) - Construction activity
13. **Business Analysis** (`business_analysis_merge_16.py`) - Competitive landscape

### **Feature Engineering**
- **Spatial Features**: Location coordinates, district analysis, neighborhood characteristics
- **Temporal Features**: Start date analysis, economic cycle alignment
- **Business Features**: Industry classification, permit history, competitive density
- **Economic Features**: GDP trends, unemployment rates, housing market indicators
- **Sentiment Features**: News sentiment analysis, economic optimism indices

---

## üìä **Data Science Methodology**

### **Data Collection & Integration**
```python
# Automated pipeline processes 13 API sources
scripts_03_13 = [
    "api_keys_validation_01.py",      # API authentication
    "sf_business_data_04.py",         # Primary business data
    "fred_economic_data_05.py",       # Economic indicators
    # ... 8 additional data sources
    "wayback_historical_data_13.py"   # Historical validation
]
```

### **Preprocessing Pipeline** (`premodeling_pipeline_23.py`)
- **Temporal Leakage Prevention**: Ensures no future data influences historical predictions
- **Data Quality Assurance**: Comprehensive cleaning and validation
- **Feature Standardization**: Robust scaling for neural network optimization
- **Train/Validation/Test Split**: Temporal-aware splitting for reliable evaluation

### **Model Training** (`model_training_with_save_load_24.py`)
- **Custom PyTorch Architecture**: Multi-branch design for different feature types
- **Class Imbalance Handling**: Weighted loss functions for realistic business scenarios
- **Early Stopping**: Prevents overfitting with validation monitoring
- **Model Persistence**: Comprehensive serialization for production deployment

---

## üõ†Ô∏è **Technology Stack**

**Machine Learning**
- PyTorch Lightning (Deep Learning Framework)
- Scikit-learn (Preprocessing & Metrics)
- NumPy/Pandas (Data Manipulation)

**Data Engineering**
- FRED API (Economic Data)
- Socrata API (Government Data)
- REST APIs (Multiple Sources)
- Parquet (High-Performance Storage)

**Web Application**
- Streamlit (Interactive Dashboard)
- Plotly (Advanced Visualizations)
- Geospatial Libraries (Location Analysis)

**DevOps & Deployment**
- Docker (Containerization)
- Docker Hub (Public Registry)
- Multi-stage Builds (Optimization)

---

## üìà **Production Features**

### **Interactive Dashboard**
- **Real-time Predictions**: Input business parameters for instant success probability
- **Geospatial Visualization**: Interactive maps showing location-based risk factors
- **Trend Analysis**: Historical economic indicator impacts
- **Comparative Analysis**: Benchmark against similar businesses

### **Model Interpretability**
- **Feature Importance**: Understand which factors drive predictions
- **Confidence Intervals**: Quantify prediction uncertainty
- **Scenario Analysis**: Test different economic conditions
- **Risk Decomposition**: Break down risk by category (economic, spatial, temporal)

### **Scalability & Reliability**
- **Containerized Deployment**: Consistent environment across platforms
- **Error Handling**: Graceful degradation for missing data
- **Performance Optimization**: Efficient inference for real-time use
- **Model Versioning**: Track model performance over time

---

## üéØ **Business Applications**

### **For Entrepreneurs**
- **Location Optimization**: Identify high-success probability areas
- **Timing Analysis**: Determine optimal launch windows
- **Risk Mitigation**: Understand and prepare for key risk factors
- **Competitive Intelligence**: Assess market saturation levels

### **For Investors**
- **Due Diligence**: Quantitative risk assessment for business investments
- **Portfolio Optimization**: Balance risk across different business types/locations
- **Market Timing**: Identify economic cycles favorable for different industries
- **Performance Benchmarking**: Compare investments against predicted outcomes

### **For Policymakers**
- **Economic Development**: Identify areas needing business development support
- **Zoning Decisions**: Understand how land use affects business success
- **Resource Allocation**: Focus business development programs effectively
- **Impact Assessment**: Measure policy effects on business ecosystem

---

## üîç **Key Insights & Findings**

The model reveals that San Francisco business success is driven by:

1. **Location Intelligence**: Specific neighborhoods show 2-3x higher success rates
2. **Economic Timing**: Businesses launched during economic upturns have 40% higher survival
3. **Industry Clustering**: Co-location with complementary businesses increases success probability
4. **Infrastructure Quality**: Areas with lower 311 service requests correlate with business success
5. **Economic Indicators**: GDP growth and unemployment rates are leading predictors

---

## üìã **Getting Started**

### **Quick Demo**
```bash
# Pull and run the containerized application
docker run -p 8501:8501 esengendo730/sf-business-model:latest

# Access the dashboard
open http://localhost:8501
```

### **Development Setup**
```bash
# Clone repository
git clone https://github.com/esengendo/sf-business-model.git
cd sf-business-model

# Install dependencies
pip install -r requirements.txt

# Run full pipeline (optional - for data updates)
python pipeline_runner.py

# Launch application
streamlit run app.py
```

---

## üéì **Technical Skills Demonstrated**

### **Data Science & ML**
- **Deep Learning**: Custom PyTorch architectures, transfer learning
- **Feature Engineering**: Multi-source data integration, temporal features
- **Model Evaluation**: Cross-validation, performance metrics, interpretability
- **Statistical Analysis**: Time series analysis, spatial statistics

### **Data Engineering**
- **API Integration**: 13 different data sources, rate limiting, error handling
- **ETL Pipelines**: Automated data collection, transformation, validation
- **Data Quality**: Comprehensive cleaning, missing value handling
- **Storage Optimization**: Parquet format, efficient data structures

### **Software Engineering**
- **Production Code**: Modular design, error handling, logging
- **Containerization**: Docker multi-stage builds, optimization
- **Testing**: Data validation, model performance monitoring
- **Documentation**: Comprehensive project documentation

### **Business Intelligence**
- **Dashboard Development**: Interactive visualizations, user experience
- **Business Metrics**: KPI definition, performance tracking
- **Stakeholder Communication**: Clear presentation of technical concepts
- **Strategic Insights**: Actionable recommendations from data analysis

---

## üìû **Contact & Portfolio**

This project demonstrates **end-to-end data science capabilities** from business problem definition through production deployment. The complete pipeline showcases skills in **data engineering**, **machine learning**, **software development**, and **business intelligence**.

**Live Demo**: `docker run -p 8501:8501 esengendo730/sf-business-model:latest`

*Built with attention to production readiness, business value, and technical excellence.* 